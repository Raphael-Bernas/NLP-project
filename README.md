# Quantizer robustness training in Generative Spoken Language Modeling
# (Project 6 for the course "Speech and natural language processing")
https://github.com/chloedaphne/MVA_2025_SNLP/

## Original paper
These works mainly rely on "Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling" by Itai Gat, Felix Kreuk et al.

## Notebook
In the notebook `NLP-project.ipynb` you can find our code used for this project.
The code is divided in 7 parts which have each been done by one of us.

## Paper
You can find our report in the form of a paper. It contains a detailed and explained walk through our works.

## Authors
RaphaÃ«l Bernas (ENSTA-MVA), Maxime Corlay (ENSTA-MVA), Adrien Letellier (ENSAE-MVA) and Emilie Zheng (ENS-MVA).


## Task repartion :

a)	To recode the encoder part to have the basics [Adrien] -> part 1

b)	To recode the quantizer part with the training method introduced in their paper [RaphaÃ«l] -> part 2

c)	To recode testing methods: UED and ABX [RaphaÃ«l] - > part 2

d)	To implement general augmentation and some more [Maxime] -> part 3

e)	To implement new augmentation (corruption and volume change) [Emilie] -> part 4

f)	To test models on the original settings [Emilie] -> part 4

g)	To test new models [Maxime] -> part 5

h)	To suggest other metric, mainly reform the UED metric and Levenshtein distance using Dynamic Time Warping [Adrien] -> part 6

i)	To test new architecture of quantizer for E1 (change the MLP based method) [Raphael] -> part 7
